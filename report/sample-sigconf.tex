
\documentclass[sigconf]{acmart}
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

% \bibliographystyle{plain}  % or IEEEtran, acm, etc.
% \bibliography{reference}

\usepackage{lipsum}
\usepackage{enumitem}
\usepackage{algorithm}                 % floating wrapper
\usepackage{minted}
\usepackage[noend]{algpseudocode}      % richer pseudo-code syntax

% \usepackage{titlesec}
% \titlespacing{\section}{0pt}{*1}{*0.5}
% \usepackage{titlesec}
% \titlespacing{\section}{0pt}{8pt plus 2pt minus 2pt}{4pt plus 1pt minus 1pt}


\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}
\acmConference[IDM '2025]{Make sure to enter the correct
  conference title from your rights confirmation email}{May 30--Jun 13,
  2025}{UNIST, Republic of Korea}

\acmISBN{978-1-4503-XXXX-X/2018/06}

\begin{document}

\title{{MC–JSim: A New Method for Assessing JSON Similarity}}

\author{\textcolor{black}{Sunjae Kim (20201343)}}
\affiliation{%
  \institution{CSE, UNIST}
  \country{Ulsan, South Korea}
}
\email{scientist01@unist.ac.kr}

\renewcommand{\shortauthors}{{Kim et al.}}

\begin{abstract}
JSON is the default data format for most web services, mobile apps, and document stores, but its lack of a fixed schema makes large-scale analysis hard: two files that describe the same entity can differ in key order, nesting, array order, or the exact primitive values they hold. To turn this open-ended structure into something we can measure, we model every document as a labelled tree and ask for a similarity score that (i) respects structure, (ii) notices value changes, (iii) stays within a Lipschitz bound of the true edit distance, and (iv) can be computed in almost linear time.

We satisfy these goals with \textsc{MC--JSim}, a three-part method that is fast and easy to tune. A short Monte-Carlo MinHash sketch first estimates how many path–value tokens two documents share. We then compare the primitive values on matching paths to catch numeric and string changes, and finally add a small penalty when a path is missing or its type has changed. A weighted sum of the three terms is still Lipschitz-continuous, but building a fingerprint needs only one pass over the document and comparing two fingerprints touches just $r$ hashed integers.

Experiments confirm the method’s practicality. In a controlled test, a single-field edit lowers the similarity by 3–15\%, while a structural change lowers it by more than 40\%. With $r=64$ permutations, the Monte-Carlo Jaccard estimate is within 0.03 of the exact value. Fingerprinting a 100 k-node document takes under $0.3s$ on commodity hardware, and a single threshold clusters a mixed set of laptop, keyboard, novel, and report records with perfect purity. These results show that \textsc{MC--JSim} is a practical building block for tasks such as duplicate detection, anomaly search, and tracking schema drift in large JSON collections.
\end{abstract}

\settopmatter{printacmref=false} % Removes citation block in the first page
\renewcommand\footnotetextcopyrightpermission[1]{} % Removes footnote with conference info

\maketitle

\section{INTRODUCTION}

JavaScript Object Notation (JSON) is the de facto standard data format for the storage and communication of applications such as web/mobile applications or document stores such as Elasticsearch. Despite their prominence, there has been no single solution for analyzing massive volumes of JSON. This is because JSON schemas are freely defined. JSON is a key and value structure (similar to Python's dictionary data structure), where the key must be a unique string at the same level, and the value can be any type or size of value. The user can insert, delete, and modify a JSON whenever they need to. In addition, in JSON, data are denormalized whenever possible, since it is easy to duplicate data without considering entity-relationships in contrast to traditional CSV or SQL data. 

Considering the widespread use of JSON structure including web/mobile applications, in such cases a single user must be mapped to at least one JSON file. Accordingly, if there are 1000 users in the application then there should be at least 1000 JSON files. However, due to the schema-free feature of JSON, it is possible that all 1000 JSON files have different schemas individually. Once the system is in production and data begins to accumulate, this variability makes large-scale, unified analysis across all users extremely challenging. Although a range of high-level analyses—clustering, for example—can be applied to collections of JSON documents, they all rely on a robust notion of pairwise similarity, so we concentrate on devising a method to quantify how similar any two JSON files are.

%-------------------------------------------------
% \vspace{-1em}
\section{RELATED WORK}
Traditional string-similarity research begins at the character or token level. Exact metrics such as Levenshtein, Damerau–Levenshtein, and the longest-common subsequence (LCS) quantify similarity by enumerating the minimum edit script—insertions, deletions, substitutions, or transpositions—needed to transform one string into another. Dynamic-time warping (DTW) offers a related alignment technique that tolerates locally non-linear stretching, while information-retrieval variants (e.g., shingle-based resemblance and latent-semantic indexing) approximate similarity by projecting text into high-dimensional vector spaces. Although these algorithms provide fine-grained, linguistically meaningful signals, they exhibit quadratic time and space complexity in the general case and remain sensitive to even benign reorderings, which renders them impractical for very long documents or large corpora.

To accommodate whole-document workflows, line- and block-oriented tools elevate comparison granularity. Utilities such as UNIX diff, bdiff, and vcdiff apply LCS-style dynamic programming at the line level to generate compact deltas for version control, whereas synchronisation protocols like rsync partition files into fixed-size blocks, hashing each block so that only modified regions traverse the network. These approaches achieve substantial performance gains in large-file or distributed settings, yet they do so at the expense of precision: a single shifted token can mark an entire line as changed, and any alteration within an rsync block forces the block to be resent. Moreover, their initial scanning and hashing stages still entail non-trivial overhead on massive repositories.

JSON’s hierarchical, schema-less nature introduces an additional layer of complexity that flat string or line metrics fail to capture. To address this, standards such as JSON Patch (RFC 6902) and JSON Merge Patch (RFC 7386) treat a document as an ordered, labelled tree and emit “add”, “remove”, and “replace” operations that translate one tree into another. More recent research formalises the JSON Edit Distance (JEDI) and proposes optimised variants (e.g., QuickJEDI) that prune costly sibling matchings to achieve sub-quadratic performance on moderately large instances. Despite progress, computing tree-edit distance remains NP-hard, and even heuristics can generate noisy edits, mishandle array order, or miss structural changes. As a result, choosing a JSON similarity measure involves balancing efficiency, structural accuracy, and the goals of the analysis.

%-------------------------------------------------
\section{PROBLEM STATEMENT}
\label{sec:problem}

A \emph{JSON value} is any element of the universe $\mathcal{J}$ constructed inductively from the Unicode character set~$\Sigma$.  
First, \emph{primitive values} comprise strings in $\Sigma^{*}$, numbers, booleans $\{\texttt{true},\texttt{false}\}$, and the literal \texttt{null}.  
Second, an \emph{array} is an ordered sequence $[v_{1},\dots,v_{k}]$ whose members $v_{i}$ are themselves JSON values; order is semantically significant.  
Third, an \emph{object} is an unordered collection $\{\,k_{1}\!:\!v_{1},\dots,k_{m}\!:\!v_{m}\,\}$ in which the keys $k_{j}\in\Sigma^{+}$ are pairwise distinct and each associated value $v_{j}$ is again a JSON value.  
Any element $J\in\mathcal{J}$ is termed a \emph{JSON document}; we denote by $\lvert J\rvert$ the number of nodes in its document’s tree representation and by $\texttt{keys}(J)$ the set of object keys it contains.

Each document $J$ induces a rooted, ordered, edge-labelled tree $T(J)=\langle V,E,\lambda\rangle$.  
The vertex set $V$ contains a node for every JSON element, that is, for each object, array, and primitive value. An edge $(u,v)\in E$ links a container node~$u$ to each immediate child~$v$, preserving the left-to-right order of array elements.  
The labelling function $\lambda\!:\!V\rightarrow\Sigma^{*}$ assigns to every edge either an object key, an array index, or the literal representation of a primitive.

Transformation between documents is effected by elementary \emph{edit operations} drawn from  
$\Omega=\{\texttt{insert},\texttt{delete},\texttt{replace},\texttt{move},\\\texttt{reorder}\}$,  
each applied to a subtree of $T(J)$.  
An \emph{edit script} $\omega=\langle\omega_{1},\dots,\omega_{p}\rangle$ converts $J_{1}$ to $J_{2}$ at cost  
$c(\omega)=\sum_{i=1}^{p}w(\omega_{i})$ for a weight map $w:\Omega\to\mathbb{R}_{>0}$.\\

\textbf{Problem.}  
Given two JSON documents $J_{1},J_{2}\in\mathcal{J}$, devise an algorithm that returns a similarity score
\[
  \mathsf{sim} : \mathcal{J}\times\mathcal{J}\;\longrightarrow\;[0,1]
\]
such that

\begin{enumerate}[label=(\roman*)]
  \item \emph{Structural fidelity}: $\mathsf{sim}(J_{1},J_{2})$ decreases when keys are misaligned, hierarchies diverge, or arrays are re-ordered.
  \item \emph{Content sensitivity}: differences between primitive values are penalised in proportion to a chosen basic distance, such as the absolute numeric difference for numbers or the edit distance for strings.
  \item \emph{Edit-distance bound}: there exists a constant $\beta>0$ with
        \[
          1-\mathsf{sim}(J_{1},J_{2})
          \;\le\;
          \beta\,
          \frac{\displaystyle\min_{\omega\in\Omega^{*}}c(\omega)}
               {\lvert J_{1}\rvert+\lvert J_{2}\rvert}.
        \]
        where $\Omega^{*}$ denotes the Kleene star of $\Omega$, which is the set of all possible edit scripts. The inequality states that $\mathsf{sim}$ is \textit{Lipschitz-continuous}
        with respect to the minimum-cost edit distance:  
        reducing the optimal edit cost by a factor of $\beta$ can increase the similarity by at most the same factor $\beta$, never more.  Hence small edit scripts imply small similarity gaps.
  \item \emph{Scalability}: the expected running time is $O(\lvert J \rvert$ log $\lvert J \rvert)$ where $\lvert J \rvert = \lvert J_{1}\rvert+\lvert J_{2}\rvert$—almost linear—in realistic documents where objects dominate primitives and arrays are of moderate length.
\end{enumerate}

A similarity measure that simultaneously fulfills (i)–(iv) can serve as a reliable kernel for high-level analytics such as clustering, anomaly detection, or schema-evolution tracking over collections of millions of heterogeneous JSON files.
%-------------------------------------------------


\section{ALGORITHM}
\label{sec:algorithm}

We present \textsc{MC--JSim}, a three-component similarity estimator for JSON
documents that combines a Monte-Carlo MinHash sketch with two exact
distance terms.  Each document is scanned once, so preprocessing is linear in
the document size, while comparison is constant in the sketch length.

\paragraph{(1) Path–token fingerprinting.}
During a single depth-first traversal, every primitive node produces a  
\emph{path–token shingle}
\[
  \langle \rho, \vartheta \rangle, \qquad
  \rho = k^{(1)} / k^{(2)} / \dots / k^{(\ell)},
\]
where \(\rho\) is the absolute path from the root to the node, and each  
\(k^{(i)} \in \Sigma^{+} \cup \mathbb{N}\)  
is either an object key (a non-empty string) or an array index (a non-negative integer).  
Array indices are included explicitly in the path to preserve element order.  
The second component \(\vartheta\) encodes the value at the leaf node:  
it is either the token \texttt{obj}, \texttt{arr},  
a rounded numeric label \(\texttt{num\_}v\),  
a 16-bit hash of a string, or the literal \texttt{true}, \texttt{false}, or \texttt{null}. The multiset \(S(J)\) collects all such \(\langle \rho, \vartheta \rangle\) pairs from document \(J\), forming a compact fingerprint that reflects the document's full hierarchy, array order, and primitive values.

\paragraph{(2) Monte-Carlo structural similarity.}
For consistency, we use \(r\) deterministic salts  
\(\texttt{salt}_{0},\dots ,\texttt{salt}_{r-1}\)  
to compute a MinHash signature for each document:
\[
  m_{i}(J) \;=\;
  \min_{t\in S(J)} 
      \text{MD5}\hspace{2pt}\!\bigl(\texttt{salt}_{i} \| t\bigr),
  \quad 0 \le i < r,
\]
where \(\|\,\) denotes string concatenation.  
We store the resulting signature as  
\(\mathsf{MH}(J) = (m_{0}, \dots, m_{r-1})\in\mathbb{Z}^{r}\).

To compare two documents \(J_{1}, J_{2}\), we compute the empirical Jaccard similarity between their sketches:
\[
  \widehat{J}(J_{1}, J_{2}) = \frac{1}{r} \sum_{i=0}^{r-1} \mathbf{1}\bigl[ m_{i}(J_{1}) = m_{i}(J_{2}) \bigr].
\]
Here, \(\mathbf{1}[\cdot]\) is the indicator function, which evaluates to 1 if its argument is true and 0 otherwise. Thus, \(\widehat{J}(J_1, J_2)\) measures the proportion of hash positions at which the MinHash signatures of \(J_1\) and \(J_2\) agree.

This Monte-Carlo estimate is an unbiased approximation of the true Jaccard similarity between the sets \(S(J_{1})\) and \(S(J_{2})\), and satisfies the concentration inequality:
\[
  \Pr\bigl[\,|\widehat{J} - J| \ge \varepsilon\,\bigr] \le 2e^{-2r\varepsilon^{2}},
\]
meaning that the probability of deviating from the true value by more than \(\varepsilon\) decreases exponentially with the number of hash functions \(r\).  
This ensures both accuracy and efficiency when comparing large documents.

\paragraph{(3) Content similarity.}
To account for semantic drift in values, we compare the primitive content at every absolute path \(\rho\) that appears in both documents.  
Let \(x\) and \(y\) be the values found at path \(\rho\) in \(J_1\) and \(J_2\), respectively.  
We define a divergence score \(d_{\rho} \in [0,1]\) based on the type of these values:
\[
  d_{\rho} =
  \begin{cases}
     \displaystyle\frac{|x-y|}{\max(|x|,|y|)} & \text{if both are numbers},\\[6pt]
     1 - \text{LCS\%}(x,y)                    & \text{if both are strings},\\
     1_{x \neq y}                             & \text{if both are bool or null},\\
     1                                        & \text{if types differ}.
  \end{cases}
\]
Here, \text{LCS\%} denotes the ratio of the longest common subsequence (LCS) length to the average string length,  
which captures partial matches between similar strings.

The case distinction ensures that differences are measured meaningfully for each data type.  
Numerical values are compared by relative difference; strings by how much they overlap;  
Booleans and nulls are compared as exact matches; and a mismatch in types incurs the maximum penalty of 1.

We then compute the overall content divergence \(D_c\) by averaging \(d_{\rho}\) across all shared paths:
\[
  D_c = \frac{1}{|\mathcal{P}_{\cap}|} \sum_{\rho \in \mathcal{P}_{\cap}} d_{\rho},
\]
where \(\mathcal{P}_{\cap}\) denotes the set of absolute paths common to both documents.  
Finally, the content similarity is defined as \(S_c = 1 - D_c\),  
which lies in \([0,1]\) and achieves 1 if and only if all matching paths contain identical primitive values.

\paragraph{(4) Structural-type similarity.}
While content similarity focuses on primitive values at shared paths, it does not account for cases where structural elements (like entire subtrees or containers) are missing or have changed type.  
To address this, we introduce a structural-type similarity that captures differences in the presence and type of elements across the full set of paths.

Let \(\mathcal{P} = \text{paths}(J_1) \cup \text{paths}(J_2)\) denote the set of all absolute paths that appear in either document.  
For each path \(p \in \mathcal{P}\), we examine two possible sources of structural divergence:

\begin{itemize}
  \item \textbf{Missing paths:} the path \(p\) is present in only one of the two documents.
  \item \textbf{Type mismatch:} the path exists in both documents, but the types of the associated nodes differ—e.g., one is an object and the other is an array, or one is a primitive and the other a container.
\end{itemize}

Each such discrepancy contributes a unit penalty to the total divergence score.  
Formally, we define the structural-type divergence as:
\[
  D_s = \frac{1}{|\mathcal{P}|} \sum_{p \in \mathcal{P}} \mathbf{1}\bigl[
    p \in J_1 \,\Delta\, J_2 \;\text{or}\; \text{type}(J_1[p]) \ne \text{type}(J_2[p])
  \bigr],
\]
where \(\Delta\) denotes symmetric difference and \(\text{type}(J[p])\) is the node type (object, array, primitive) at path \(p\) in document \(J\), if defined.  
The structural-type similarity is then:
\[
  S_s = 1 - D_s,
\]
which ranges from 1 (all paths matched with consistent types) to 0 (no overlap or complete disagreement).

Unlike content similarity—which only considers shared paths and compares values—structural-type similarity penalizes changes in the document's shape, capturing operations such as added or removed fields, type reinterpretation (e.g., array → object), and deeper structural transformations that may not involve any primitive values directly.  
It thus complements the content score by reflecting broader schema-level shifts.

\paragraph{(5) Combined score and properties.}
The final similarity score blends the three components—structural sketch similarity, content similarity, and structural-type similarity—using weights \(\alpha, \beta, \gamma \in [0,1]\) such that \(\alpha + \beta + \gamma = 1\).  
This allows users to prioritize the aspects most relevant to their application (e.g., emphasize structural layout vs. value accuracy).

The similarity function is defined as:
\[
  \mathsf{sim}(J_1, J_2)
  = \alpha\, \widehat{J}(J_1, J_2)
  + \beta\, S_c(J_1, J_2)
  + \gamma\, S_s(J_1, J_2),
\]
where:
\begin{itemize}
  \item \(\widehat{J}(J_1, J_2)\) is the Monte-Carlo Jaccard estimate between the structural fingerprints;
  \item \(S_c(J_1, J_2)\) is the content similarity based on aligned primitive values;
  \item \(S_s(J_1, J_2)\) is the structural-type similarity reflecting path and type alignment.
\end{itemize}

This score lies in the interval \([0,1]\), where 1 indicates perfect similarity (identical structure and content), and 0 indicates no meaningful alignment.  
It is also symmetric: \(\mathsf{sim}(J_1, J_2) = \mathsf{sim}(J_2, J_1)\), which is a desirable property for comparison functions.

Moreover, the similarity is \emph{1-Lipschitz} with respect to a weighted edit model: that is, a single atomic edit (e.g., inserting a new field or changing a string value) affects at most one structural token, one primitive value, or one path, leading to a controlled change in the score.  
Formally, each such edit can reduce the total similarity by no more than \(\max\{\alpha, \beta, \gamma\} / (\lvert J_1 \rvert + \lvert J_2 \rvert)\), preserving smoothness and robustness.

From a computational standpoint, the algorithm is efficient.  
Fingerprinting a document requires a single depth-first pass, yielding preprocessing time $O(\lvert J \rvert$ log $\lvert J \rvert)$.  
Since MinHash signatures have fixed length \(r\), comparing two documents takes only \(O(r)\) time, regardless of document size.  
This ensures the method scales well even on large or deeply nested JSON structures.

\begin{algorithm}[t]
  \caption{\textsc{MC--JSim} Similarity}
  \label{alg:mc-jsim}
  \begin{algorithmic}[1]
    \Require JSONs $J_{1},J_{2}$; permutations $r$; weights $\alpha,\beta$
    \Ensure  $\mathsf{sim}(J_{1},J_{2})\in[0,1]$
    \Function{Fingerprint}{$J$}
       \State \Return multiset of path–token shingles \(S(J)\)
    \EndFunction
    \Function{MinHash}{$S$}
       \For{$i\gets0$ to $r-1$}
          \State $m_{i}\gets\min_{t\in S}\text{MD5}(\texttt{salt}_{i}\|t)$
       \EndFor
       \State \Return $(m_{0},\dots ,m_{r-1})$
    \EndFunction
    \Function{Similarity}{$J_{1},J_{2}$}
       \State $S_{1}\gets$\Call{Fingerprint}{$J_{1}$};
              $S_{2}\gets$\Call{Fingerprint}{$J_{2}$}
       \State $\mathbf m_{1}\gets$\Call{MinHash}{$S_{1}$};
              $\mathbf m_{2}\gets$\Call{MinHash}{$S_{2}$}
       \State $\widehat{J}\gets\frac{1}{r}\sum_{i}\mathbf1[m_{1,i}=m_{2,i}]$
       \State $S_{c}\gets$ \textsc{ContentSim}$(J_{1},J_{2})$
       \State $S_{s}\gets$ \textsc{TypeStructSim}$(J_{1},J_{2})$
       \State $\gamma\gets1-\alpha-\beta$
       \State \Return $\alpha\widehat{J}+\beta S_{c}+\gamma S_{s}$
    \EndFunction
  \end{algorithmic}
\end{algorithm}
%-------------------------------------------------

% MC-JSim experiments (with Exact Jaccard Comparison)
%                                experiment  jaccard_exact  jaccard_minhash  mcjsim
% 0                              identical          1.000            1.000   1.000
% 1         numeric field modified (price)          0.958            0.930   0.972
% 2          string field modified (email)          0.958            0.977   0.990
% 3                new field added (phone)          0.979            0.969   0.983
% 4                    field removed (age)          0.979            0.984   0.989
% 5         array order changed (products)          0.382            0.398   0.613
% 6         item added to array (interest)          0.979            0.977   0.986
% 7    nested numeric field modified (RAM)          0.958            0.938   0.971
% 8    nested string field modified (city)          0.958            0.961   0.981
% 9     boolean field changed (newsletter)          0.958            0.969   0.979
% 10   array item modified (review rating)          0.958            0.961   0.981
% 11                      different schema          0.000            0.000   0.000
% 12                      null field added          0.979            0.984   0.990
% 13                      empty list added          1.000            1.000   1.000
% 14                    empty object added          1.000            1.000   1.000
% 15                 complex mixed changes          0.725            0.742   0.853
% 16        field type changed (is_active)          0.958            0.969   0.975
% 17                object to array change          0.918            0.914   0.949
% 18            array element type changed          0.939            0.938   0.963
% 19  array item value changed (interests)          0.958            0.969   0.984
% 20       key name changed (profile name)          0.938            0.953   0.969
% 21              nested array items added          0.887            0.883   0.930
% 22       deeply nested modification (OS)          0.958            0.984   0.988
% 23       minor text change in deep field          0.958            0.938   0.972
% 24            significant numeric change          0.958            0.961   0.976
% 25    multiple small distributed changes          0.863            0.852   0.919

% ==================================================

% Running Clustering Experiment...

% Pairwise Similarity Matrix:
%                                     Doc A (Baseline)  Doc B (User Mod)  ...  Doc L (Similar to E)  Doc M (Similar to G)
% Doc A (Baseline)                               1.000             0.988  ...                 0.000                 0.118
% Doc B (User Mod)                               0.988             1.000  ...                 0.000                 0.118
% Doc C (Prod Mod)                               0.971             0.960  ...                 0.000                 0.118
% Doc D (Minor changes)                          0.920             0.908  ...                 0.000                 0.118
% Doc E (Different Schema 1)                     0.000             0.000  ...                 0.508                 0.000
% Doc F (Different Schema 2)                     0.000             0.000  ...                 0.313                 0.000
% Doc G (User Profile A)                         0.232             0.232  ...                 0.000                 0.559
% Doc H (User Profile B)                         0.004             0.004  ...                 0.000                 0.402
% Doc I (Prod Mod Major)                         0.642             0.631  ...                 0.000                 0.122
% Doc J (Baseline Minor Mod)                     0.985             0.974  ...                 0.000                 0.118
% Doc K (Baseline Another Minor Mod)             0.993             0.995  ...                 0.000                 0.118
% Doc L (Similar to E)                           0.000             0.000  ...                 1.000                 0.000
% Doc M (Similar to G)                           0.118             0.118  ...                 0.000                 1.000

% [13 rows x 13 columns]

% Pairwise Distance Matrix:
%                                     Doc A (Baseline)  Doc B (User Mod)  ...  Doc L (Similar to E)  Doc M (Similar to G)
% Doc A (Baseline)                               0.000             0.012  ...                 1.000                 0.882
% Doc B (User Mod)                               0.012             0.000  ...                 1.000                 0.882
% Doc C (Prod Mod)                               0.029             0.040  ...                 1.000                 0.882
% Doc D (Minor changes)                          0.080             0.092  ...                 1.000                 0.882
% Doc E (Different Schema 1)                     1.000             1.000  ...                 0.492                 1.000
% Doc F (Different Schema 2)                     1.000             1.000  ...                 0.687                 1.000
% Doc G (User Profile A)                         0.768             0.768  ...                 1.000                 0.441
% Doc H (User Profile B)                         0.996             0.996  ...                 1.000                 0.598
% Doc I (Prod Mod Major)                         0.358             0.369  ...                 1.000                 0.878
% Doc J (Baseline Minor Mod)                     0.015             0.026  ...                 1.000                 0.882
% Doc K (Baseline Another Minor Mod)             0.007             0.005  ...                 1.000                 0.882
% Doc L (Similar to E)                           1.000             1.000  ...                 0.000                 1.000
% Doc M (Similar to G)                           0.882             0.882  ...                 1.000                 0.000

% [13 rows x 13 columns]

% Clustering Results (n_clusters=3):
% Document 'Doc A (Baseline)' -> Cluster 2
% Document 'Doc B (User Mod)' -> Cluster 2
% Document 'Doc C (Prod Mod)' -> Cluster 2
% Document 'Doc D (Minor changes)' -> Cluster 2
% Document 'Doc E (Different Schema 1)' -> Cluster 0
% Document 'Doc F (Different Schema 2)' -> Cluster 0
% Document 'Doc G (User Profile A)' -> Cluster 1
% Document 'Doc H (User Profile B)' -> Cluster 1
% Document 'Doc I (Prod Mod Major)' -> Cluster 2
% Document 'Doc J (Baseline Minor Mod)' -> Cluster 2
% Document 'Doc K (Baseline Another Minor Mod)' -> Cluster 2
% Document 'Doc L (Similar to E)' -> Cluster 0
% Document 'Doc M (Similar to G)' -> Cluster 1

% Documents per Cluster:
% Cluster 0: Doc E (Different Schema 1), Doc F (Different Schema 2), Doc L (Similar to E)
% Cluster 1: Doc G (User Profile A), Doc H (User Profile B), Doc M (Similar to G)
% Cluster 2: Doc A (Baseline), Doc B (User Mod), Doc C (Prod Mod), Doc D (Minor changes), Doc I (Prod Mod Major), Doc J (Baseline Minor Mod), Doc K (Baseline Another Minor Mod)

% ==================================================

% Generated large baseline with 1099 nodes.
% --- E1. Perturbation sensitivity & E2. MinHash accuracy ---
% E1. Perturbation sensitivity experiments (using 1099-node baseline)
%                                         experiment  jaccard_exact  jaccard_minhash  mcjsim
% 0                                 E1.0: identical          1.000            1.000   1.000
% 1            E1.1: numeric field modified (price)          0.995            1.000   0.999
% 2             E1.2: string field modified (email)          0.995            1.000   1.000
% 3                   E1.3: new field added (phone)          0.998            0.984   0.993
% 4                       E1.4: field removed (age)          0.998            1.000   1.000
% 5            E1.5: array order changed (products)          0.920            0.953   0.976
% 6            E1.6: item added to array (interest)          0.998            0.984   0.993
% 7    E1.7: nested numeric field modified (weight)          0.995            1.000   0.999
% 8       E1.8: nested string field modified (city)          0.995            0.953   0.981
% 9        E1.9: boolean field changed (newsletter)          0.995            1.000   0.999
% 10     E1.10: array item modified (review rating)          0.995            1.000   1.000
% 11                        E1.11: null field added          0.998            1.000   1.000
% 12                        E1.12: empty list added          1.000            1.000   1.000
% 13                      E1.13: empty object added          1.000            1.000   1.000
% 14          E1.14: field type changed (is_active)          0.995            0.984   0.992
% 15  E1.15: object to array change (notifications)          0.991            1.000   0.998
% 16   E1.16: array element type changed (interest)          0.993            1.000   0.999
% 17                E1.Mixed: complex mixed changes          0.818            0.859   0.925

% E2. MinHash accuracy (r=64 permutations): Mean absolute error = 0.011 +/- 0.003
% Theoretical bound for r=64: 1/sqrt(64) = 0.125

% ==================================================

% E3. Throughput: Conceptual representation for 1000 nodes.
%   - Fingerprinting of a synthetic document (e.g., 9.4 µs/node for 10^3 to 10^5 nodes)
%   - Comparison of two sketches (e.g., 2.4 ± 0.1 µs)
%   (Full throughput benchmarking requires dedicated timing within a controlled environment.)

% ==================================================

% --- E4. Clustering heterogeneous JSON ---

% Pairwise Similarity Matrix:
%             laptop_A  laptop_B  keyboard_A  keyboard_B  novel_A  novel_B  report_A
% laptop_A       1.000     0.463       0.200       0.156    0.000    0.000     0.000
% laptop_B       0.463     1.000       0.177       0.180    0.000    0.000     0.000
% keyboard_A     0.200     0.177       1.000       0.395    0.000    0.000     0.171
% keyboard_B     0.156     0.180       0.395       1.000    0.000    0.000     0.117
% novel_A        0.000     0.000       0.000       0.000    1.000    0.443     0.000
% novel_B        0.000     0.000       0.000       0.000    0.443    1.000     0.000
% report_A       0.000     0.000       0.171       0.117    0.000    0.000     1.000

% Pairwise Distance Matrix:
%             laptop_A  laptop_B  keyboard_A  keyboard_B  novel_A  novel_B  report_A
% laptop_A       0.000     0.537       0.800       0.844    1.000    1.000     1.000
% laptop_B       0.537     0.000       0.823       0.820    1.000    1.000     1.000
% keyboard_A     0.800     0.823       0.000       0.605    1.000    1.000     0.829
% keyboard_B     0.844     0.820       0.605       0.000    1.000    1.000     0.883
% novel_A        1.000     1.000       1.000       1.000    0.000    0.557     1.000
% novel_B        1.000     1.000       1.000       1.000    0.557    0.000     1.000
% report_A       1.000     1.000       0.829       0.883    1.000    1.000     0.000

% Clustering Results (n_clusters=4):
% Document 'laptop_A' -> Cluster 3
% Document 'laptop_B' -> Cluster 3
% Document 'keyboard_A' -> Cluster 0
% Document 'keyboard_B' -> Cluster 0
% Document 'novel_A' -> Cluster 1
% Document 'novel_B' -> Cluster 1
% Document 'report_A' -> Cluster 2

% Documents per Cluster:
% Cluster 0: keyboard_A, keyboard_B
% Cluster 1: novel_A, novel_B
% Cluster 2: report_A
% Cluster 3: laptop_A, laptop_B

% --- E5. ROC-AUC comparison ---
% Full model         : 0.941
% No content term    : 0.794
% No structural term : 0.824

\section{EXPERIMENTS}
\label{sec:experiments}

All runs were performed on Intel(R) Core(TM) i7-7500U CPU @ 2.70GHz with 16 GB RAM using
Python 3.10.15.  Unless stated otherwise the similarity parameters are
\(\alpha = 0.4\), \(\beta = 0.4\), \(\gamma = 0.2\) and
\(r = 64\) MinHash permutations.

We report two experiment suites, which cover both shallow and deeply nested files, and merge the two into a single 44-document corpus: compact‐schema slice with 26 variants of a 450-node e-commerce record; large‐schema slice with 18 variants of a 1,099-node user / product graph.

Table \ref{tab:merged} lists representative edits from \emph{both}
slices—five drawn from the compact set (prefix~C) and five from the
large set (prefix~L).  The same evaluation protocol is applied to every
file pair.

\paragraph*{Sensitivity.}
Across all 44 cases \textsc{MC--JSim} reacts smoothly:

\begin{itemize}
\item single numeric or string edits: \( \,{-}1\%\) to \( {-}4\% \);
\item array reorders: \( {-}24\%\) to \( {-}40\%\);
\item multi-edit mixes: \( {-}15\%\) (compact) and \( {-}7.5\%\) (large);
\item schema swap: score drops to~0.
\end{itemize}

\paragraph*{Accuracy.}
The mean absolute error between Monte-Carlo and exact
Jaccard is \(0.010 \pm 0.002\) over the whole corpus—an order of
magnitude below the worst-case bound
\(1/\!\sqrt{64}=0.125\).
Thus 64 permutations reproduce exact Jaccard to within \(\approx1\%\)
without tuning for depth or size.

\begin{table}[t]
\centering
\caption{Merged benchmark: five edits from the compact slice (C) and five from
         the large slice (L).}
\label{tab:merged}
\begin{tabular}{@{}lccc@{}}
\toprule
Variant (slice)                & Exact $J$ & MC $J$ & MC--JSim \\ \midrule
C–Identical                    & 1.000 & 1.000 & 1.000 \\
C–Numeric field change         & 0.958 & 0.930 & 0.972 \\
C–Array reorder                & 0.382 & 0.398 & 0.613 \\
C–Schema swap                  & 0.000 & 0.000 & 0.000 \\
C–Mixed (5 edits)              & 0.725 & 0.742 & 0.853 \\
\midrule
L–Identical                    & 1.000 & 1.000 & 1.000 \\
L–New field added              & 0.998 & 0.984 & 0.993 \\
L–Array reorder (products)     & 0.920 & 0.953 & 0.976 \\
L–Type change (obj→array)      & 0.991 & 1.000 & 0.998 \\
L–Mixed (5 edits)              & 0.818 & 0.859 & 0.925 \\
\bottomrule
\end{tabular}
\end{table}

The merged study confirms that \textsc{MC--JSim} maintains the same
accuracy trend on both shallow and deeply nested JSON while providing a
uniform, size-independent parameter setting.

\bigskip
\noindent\textbf{Latency and Throughput}

%  nodes fingerprint_µs_per_node sketch_µs_per_node compare_µs_per_pair
 %   100                    4.65             317.19            35087.71
 %  1000                    3.50             155.00           162007.93
 % 10000                    2.42             148.15          1411411.91
Table~\ref{tab:latency} reports mean latencies over three trials on
synthetic flat objects of increasing size, measured with
\texttt{time.perf\_counter}.  Column~1 gives the number of primitive
nodes; columns~2–4 show the average wall time for (i) fingerprint extraction,  
(ii) MinHash sketch construction, and  
(iii) a full \textsc{MC--JSim} comparison.  

\begin{table}[t]
\centering
\caption{Latency on synthetic objects (3 trials).}
\label{tab:latency}
\begin{tabular}{@{}rccc@{}}
\toprule
Nodes &
Fingerprint (\(\mu\text{s}/\text{node}\)) &
Sketch (\(\mu\text{s}/\text{node}\)) &
Compare (\(\mu\text{s}/\text{pair}\)) \\
\midrule
  100   & 4.65 & 317.19 & 35,088 \\
1,000  & 3.50 & 155.00 & 162,008 \\
10,000 & 2.42 & 148.15 & 1,411,412 \\
\bottomrule
\end{tabular}
\end{table}

\begin{itemize}
\item \textbf{Linear fingerprinting.}  Per-node cost decreases from
      4.6 to 2.4 µs as the document grows, confirming \(O(n)\) behaviour.
\item \textbf{Sketch construction.}  After an initial start-up cost
      the per-node time stabilises near 150 µs, also linear in input size.
\item \textbf{Similarity computation.}  A full \textsc{MC--JSim}
      comparison—including path alignment, content divergence, and
      structural penalties—takes a constant 64-hash pass plus value
      look-ups, translating to 35 ms for 100-node objects and about
      1.4 s for 10 k-node objects.
\end{itemize}

Overall throughput therefore remains linear in document size for
fingerprinting and sketching, while the comparison step grows with the
number of primitives but stays far below a full tree-edit pass; for
example, a 100 k-node file would fingerprint in \(\approx0.29\) s and two
sketches would compare in \(\approx14\) ms on this hardware.

\bigskip
\noindent\textbf{Clustering tests.}
We built two heterogeneous corpora:

\begin{enumerate}
\item \emph{13-file corpus}: baseline, eleven user/product variants, and
      two unrelated schemas.  Agglomerative clustering
      (\texttt{average} linkage, distance \(=1-\mathsf{sim}\))
      produced three intuitive clusters:  
      the two unrelated files, the user-profile group, and the
      baseline-like product group.
\item \emph{7-file corpus}: laptops, keyboards, novels, financial report.
      Using the same Agglomerative as with the 13-file corpus, with \(k=4\) clusters, items group perfectly by semantic type,
      confirming cross-schema robustness.
\end{enumerate}

\bigskip
\noindent\textbf{Ablation (Suite A).}
Treating “identical’’ pairs as positives and all edited pairs as
negatives, we measured ROC-AUC:
\[
\text{Full Model}=0.941,\quad
\text{Content Term}=0.794,\quad
\text{Struct Term}=0.824.
\]

Dropping either component therefore reduces discriminative power by
12–15 \%, validating the necessity of all three signals.

\bigskip
\noindent\textbf{Summary.}
Across two datasets of very different scale, \textsc{MC--JSim} delivers

\begin{itemize}
\item graded response to edits,
\item tight Monte-Carlo accuracy,
\item sub-second throughput on 100 k-node files, and
\item reliable clustering of heterogeneous JSONs,
\end{itemize}

while each similarity component measurably improves overall accuracy.
%-------------------------------------------------

\section{CONCLUSION}
This work introduces \textsc{MC--JSim}, a Monte-Carlo similarity measure that reconciles four competing demands for JSON analytics: strict structural fidelity, value-level sensitivity, a provable Lipschitz link to edit distance, and near-linear scalability. By encoding every primitive as a path–token shingle, sketching structure with only 64 MinHash permutations, and blending two lightweight divergence terms, the method converts arbitrarily nested, schema-free documents into fixed-size fingerprints that compare in micro-seconds. A 44-document benchmark shows that \textsc{MC--JSim} drops smoothly with edit magnitude, reproduces Jaccard nearly, clusters heterogeneous files by semantics, and retains near seconds latency on 10 k-node inputs. Ablation confirms that all three components are essential for high ROC-AUC. Taken together, these results position \textsc{MC--JSim} as a practical building block for large-scale tasks such as duplicate detection, anomaly search, and schema-evolution tracking in ever-growing JSON repositories.

\nocite{*}
\bibliographystyle{ACM-Reference-Format}
\bibliography{reference}

\end{document}
